#!/usr/bin/env python3

import ipaddress
import json
import maxminddb
import sqlite3
import sys

from typing import Any, Set
from typing import Dict
from typing import List
from typing import Tuple


class Main:
    "This is the main class."

    def __init__(self, config_file: str):
        self._mmdb = maxminddb.open_database("dbip-asn-lite-2021-09.mmdb")
        self._config = json.load(open(config_file))
        self._conn = sqlite3.connect(self._config["database"])
        self._data: Dict[Tuple[str, str], Any] = {}
        self._stats: Dict[str, int] = {}
        cur = self._conn.execute("select backend, target, json from results;")
        for entry in cur:
            backend: str = str(entry[0])
            target: str = str(entry[1])
            self._data[(backend, target)] = json.loads(entry[2])

    def run(self):
        "walks the test lists and checks whether the two THs agree."
        for target_url in open(self._config["input_file"]):
            target_url = target_url.strip()
            self._increment_counter_for("processing/total")
            try:
                self._run_with_url(target_url)
            except Exception as exc:
                self._log("{}: {}".format(target_url, exc))
                self._increment_counter_for("processing/exceptions")
            else:
                self._increment_counter_for("processing/succeeded")
        json.dump(self._stats, sys.stdout, sort_keys=True)

    def _run_with_url(self, target_url: str):
        old_id = (self._config["helpers"]["old"], target_url)
        new_id = (self._config["helpers"]["new"], target_url)
        if old_id not in self._data and new_id not in self._data:
            self._increment_counter_for("processing/still_missing")
            return
        if old_id not in self._data:
            self._increment_counter_for("processing/mismatch/missing_oldth_measurement")
            return
        old_data = self._data[old_id]
        if new_id not in self._data:
            self._increment_counter_for("processing/mismatch/missing_newth_measurement")
            return
        self._increment_counter_for("processing/match/has_both_measurements")
        new_data = self._data[new_id]
        self._compare_tcp_connect(target_url, old_data, new_data)
        self._compare_http_request(target_url, old_data, new_data)
        self._compare_dns(target_url, old_data, new_data)

    def _increment_counter_for(self, key: str) -> None:
        self._stats.setdefault(key, 0)
        self._stats[key] += 1

    def _log(self, *args: Any) -> None:
        print(*args, file=sys.stderr)

    def _compare_tcp_connect(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("tcp_connect/total")
        old_data, new_data = old_data["tcp_connect"], new_data["tcp_connect"]
        old_data = self._filter_tcp_connect(old_data)
        new_data = self._filter_tcp_connect(new_data)
        diff: Any = []
        for epnt, result in old_data.items():
            if epnt not in new_data:
                self._increment_counter_for("tcp_connect/mismatch/removed_ivp4")
                diff.append(("-", epnt))
            elif result != new_data[epnt]:
                self._increment_counter_for("tcp_connect/mismatch/different_ivp4")
                diff.append(("!", epnt))
        for epnt, result in new_data.items():
            if epnt not in old_data:
                self._increment_counter_for("tcp_connect/mismatch/added_ivp4")
                diff.append(("+", epnt))
        if not diff:
            self._increment_counter_for("tcp_connect/match")
            return
        self._log("tcp connect mismatch", target_url, "***'", old_data, "'***", "***'", new_data, "'***")
        self._increment_counter_for("tcp_connect/mismatch")

    def _filter_tcp_connect(self, data: Any) -> Any:
        out: Dict[Any, Any] = {}
        for k, v in data.items():
            if k.startswith("["):
                # Here we are skipping IPv6 addresses _because_ we know
                # the old test helper cannot handle them.
                continue
            out[k] = v
        return out

    def _compare_http_request(self, target_url: str, old_data: Any, new_data: Any):
        old_data, new_data = old_data["http_request"], new_data["http_request"]
        if old_data == new_data:
            return
        self._compare_http_body_length(target_url, old_data, new_data)
        self._compare_http_failure(target_url, old_data, new_data)
        self._compare_http_title(target_url, old_data, new_data)
        self._compare_http_headers(target_url, old_data, new_data)
        self._compare_http_status_code(target_url, old_data, new_data)

    def _compare_http_body_length(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("http_body_length/total")
        old_data, new_data = old_data["body_length"], new_data["body_length"]
        if old_data == new_data:
            return
        self._increment_counter_for("http_body_length/mismatch")
        self._log("body length diff for {}: old={} => new={}".format(target_url, old_data, new_data))
        if old_data > new_data:
            self._increment_counter_for("http_body_length/mismatch/new_th_smaller")
        else:
            self._increment_counter_for("http_body_length/mismatch/new_th_larger")

    def _compare_http_failure(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("http_failure/total")
        old_data, new_data = old_data["failure"], new_data["failure"]
        if old_data == new_data:
            self._increment_counter_for("http_failure/match")
            return
        self._increment_counter_for("http_failure/mismatch")
        if old_data is None and new_data is not None:
            self._increment_counter_for("http_failure/mismatch/old_is_none")
            return
        if old_data is not None and new_data is None:
            self._increment_counter_for("http_failure/mismatch/new_is_none")
            return
        self._increment_counter_for("http_failure/mismatch/other")
        #self._log("http failure mismatch for {}: old={} => new={}".format(target_url, old_data, new_data))

    def _compare_http_title(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("http_title/total")
        old_data, new_data = old_data["title"], new_data["title"]
        if old_data == "" and new_data == "":
            self._increment_counter_for("http_title/match/both_empty")
            return
        if old_data == new_data:
            self._increment_counter_for("http_title/match/equal")
            return
        #self._log("title mismatch", target_url, "'", old_data, "'", "'", new_data, "'")
        self._increment_counter_for("http_title/mismatch/total")
        if old_data == "":
            self._increment_counter_for("http_title/mismatch/old_empty")
            return
        if new_data == "":
            self._increment_counter_for("http_title/mismatch/new_empty")
            return
        if len(old_data) < len(new_data):
            self._increment_counter_for("http_title/mismatch/old_th_smaller")
            return
        self._increment_counter_for("http_title/mismatch/other")

    def _compare_http_headers(self, target_url: str, old_data: Any, new_data: Any):
        # Implementation note: here we only compare the header keys under
        # the (reasonable?) assumption it is hard to compare values
        self._increment_counter_for("http_headers/total")
        old_data, new_data = old_data["headers"], new_data["headers"]
        if old_data is None and new_data is None:
            self._increment_counter_for("http_headers/match/both_none")
            return
        if old_data is None:
            self._increment_counter_for("http_headers/mismatch/old_is_none")
            return
        if new_data is None:
            self._increment_counter_for("http_headers/mismatch/new_is_none")
            return
        old_data = set([k.lower() for k in old_data.keys()])
        new_data = set([k.lower() for k in new_data.keys()])
        common_headers = set([
		    "date",
		    "content-type",
		    "server",
		    "cache-control",
		    "vary",
		    "set-cookie",
	    	"location",
		    "expires",
		    "x-powered-by",
		    "content-encoding",
	    	"last-modified",
	    	"accept-ranges",
	    	"pragma",
	    	"x-frame-options",
	    	"etag",
	    	"x-content-type-options",
	    	"age",
	    	"via",
	    	"p3p",
	    	"x-xss-protection",
	    	"content-language",
	    	"cf-ray",
	    	"strict-transport-security",
	    	"link",
	    	"x-varnish",
        ])
        # We limit comparison to just the headers that probes
        # will consider to choose whether there's blocking
        old_data = old_data - common_headers
        new_data = new_data - common_headers
        uncommon_headers_that_feels_safe_filtering = set([
            "content-length",
            "connection",
        ])
        old_data = old_data - uncommon_headers_that_feels_safe_filtering
        new_data = new_data - uncommon_headers_that_feels_safe_filtering
        header_diff = old_data ^ new_data
        if not header_diff:
            self._increment_counter_for("http_headers/match/same_set")
            return
        self._increment_counter_for("http_headers/mismatch/set_diff")
        #self._log(target_url, header_diff, file=sys.stderr)

    def _compare_http_status_code(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("http_status_code/total")
        old_data, new_data = old_data["status_code"], new_data["status_code"]
        if old_data == new_data:
            self._increment_counter_for("http_status_code/match")
            return
        self._increment_counter_for("http_status_code/mismatch")
        #self._log(target_url, old_data, new_data)

    def _compare_dns(self, target_url: str, old_data: Any, new_data: Any):
        self._increment_counter_for("dns/total")
        if old_data["dns"]["addrs"] is None:
            self._increment_counter_for("dns/mismatch/old_data_none_addrs")
            return
        if new_data["dns"]["addrs"] is None:
            self._increment_counter_for("dns/mismatch/new_data_none_addrs")
            return
        old_data = old_data["dns"]["addrs"]
        new_data = new_data["dns"]["addrs"]
        old_data = self._clean_dns_data(old_data)
        new_data = self._clean_dns_data(new_data)
        old_data, new_data = sorted(old_data), sorted(new_data)
        if old_data == new_data:
            self._increment_counter_for("dns/match/exactly")
            return
        try:
            diff = self._asn_intersection(old_data, new_data)
        except ValueError:
            self._increment_counter_for("dns/mismatch/cannot_map_to_asn")
            return
        if not diff:
            self._increment_counter_for("dns/match/same_asn")
            return
        try:
            diff = self._org_intersection(old_data, new_data)
        except ValueError:
            self._increment_counter_for("dns/mismatch/cannot_map_to_org")
            return
        if not diff:
            self._increment_counter_for("dns/match/same_org")
            return
        self._increment_counter_for("dns/mismatch/other")
        only_old_data = set(old_data) - set(new_data)
        only_new_data = set(new_data) - set(old_data)
        #
        # Important note regarding the differences I see. I was
        # under the assumption that the test helper was using
        # 8.8.8.8:53 to resolve domain names (why?). Though from
        # what I can gather (https://git.io/Ju0YR) it is using
        # the default DNS resolver. This fact _may_ explain a few
        # persistent differences that I see in my sample data,
        # which is currently list-3.txt.
        #
        # Here is an example session from the box from which I
        # am currently running tests:
        #
        # ```
        # sbs@localhost ~> host 97dounai.top
        # 97dounai.top has address 108.160.165.8
        # 97dounai.top has IPv6 address 2001::4b7e:73c0
        # sbs@localhost ~> host 97dounai.top 8.8.8.8
        # Using domain server:
        # Name: 8.8.8.8
        # Address: 8.8.8.8#53
        # Aliases:
        #
        # 97dounai.top has address 104.244.46.244
        # 97dounai.top has IPv6 address 2001::a27d:2005
        # Host 97dounai.top not found: 2(SERVFAIL)
        # ```
        #
        # Apart from the very interesting SERVFAIL error (why?)
        # the result we get here is consistent with the current output
        # of this script for the same host:
        #
        # ```
        # dns/diffs http://97dounai.top/
        #   only_old_data ['108.160.165.139@AS19679']
        #   only_new_data ['104.244.46.71@AS13414']
        # ```
        #
        # In fact, 104.244.46.244 is part of AS13414 and
        # 108.160.165.8 is part of AS19679.
        #
        # This seems to me a case where an upstream resolver
        # is telling lies to our recursive resolver, which
        # _interestingly_ is something Federico mentioned to
        # me exactly today. This requires digging in!
        #
        # See https://github.com/ooni/probe/issues/1780
        #
        '''
        self._log(
            "dns/diffs",
            target_url,
            "\n\tonly_old_data",
            self._annotate_with_asn_org(only_old_data),
            "\n\tonly_new_data",
            self._annotate_with_asn_org(only_new_data),
        )
        '''

    def _annotate_with_asn_org(self, data: Set[str]) -> List[str]:
        out: List[str] = []
        for entry in data:
            asn = self._mmdb.get(entry)
            if asn is None:
                out.append("{}@AS0".format(entry))
                continue
            out.append("{}@AS{}".format(entry, asn["autonomous_system_number"]))
        return out

    def _org_intersection(self, left: List[str], right: List[str]) -> Set[str]:
        left_set = self._build_org_mapping(left)
        right_set = self._build_org_mapping(right)
        return left_set ^ right_set

    def _build_org_mapping(self, source: List[str]) -> Set[str]:
        out: Set[str] = set()
        for entry in source:
            asn = self._mmdb.get(entry)
            if asn is None:
                raise ValueError
            asn = asn["autonomous_system_organization"]
            out.add(asn)
        return out

    def _asn_intersection(self, left: List[str], right: List[str]) -> Set[int]:
        left_set = self._build_asn_mapping(left)
        right_set = self._build_asn_mapping(right)
        return left_set ^ right_set

    def _build_asn_mapping(self, source: List[str]) -> Set[int]:
        out: Set[int] = set()
        for entry in source:
            asn = self._mmdb.get(entry)
            if asn is None:
                raise ValueError
            asn = asn["autonomous_system_number"]
            out.add(asn)
        return out

    def _clean_dns_data(self, data: List[str]) -> List[str]:
        out: List[str] = []
        for entry in data:
            try:
                ipaddress.ip_address(entry)
            except ValueError:
                pass  # skip domains
            else:
                if ":" not in entry:  # skip IPv6
                    out.append(entry)
        return out


def main():
    "Main function."
    if len(sys.argv) != 2:
        sys.exit("usage: ./compare <config-file>")
    main = Main(sys.argv[1])
    main.run()


if __name__ == "__main__":
    main()
